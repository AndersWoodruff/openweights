🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
🦥 Unsloth Zoo will now patch everything to make training faster!
INFO 06-12 08:04:20 [__init__.py:243] Automatically detected platform cuda.
Unsloth 2025.6.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
Connected to org:  CLR
==((====))==  Unsloth 2025.6.1: Fast Qwen2 patching. Transformers: 4.52.4. vLLM: 0.9.0.1.
   \\   /|    NVIDIA H100 NVL. Num GPUs = 1. Max memory: 93.096 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.7.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.3.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Creating new LoRA adapter

Map:   0%|          | 0/9 [00:00<?, ? examples/s]
Map: 100%|██████████| 9/9 [00:00<00:00, 1496.48 examples/s]

Map:   0%|          | 0/1 [00:00<?, ? examples/s]
Map: 100%|██████████| 1/1 [00:00<00:00, 197.98 examples/s]

Map:   0%|          | 0/9 [00:00<?, ? examples/s]
Map: 100%|██████████| 9/9 [00:00<00:00, 523.07 examples/s]

Map:   0%|          | 0/1 [00:00<?, ? examples/s]
Map: 100%|██████████| 1/1 [00:00<00:00, 232.84 examples/s]
/tmp/tmpckzqwlxr/sft.py:139: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedSFTTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args, **kwargs)
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 9 | Num Epochs = 100 | Total steps = 100
O^O/ \_/ \    Batch size per device = 10 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (10 x 1 x 1) = 10
 "-____-"     Trainable parameters = 18,464,768/1,795,552,768 (1.03% trained)
Train dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'token_weights': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}
Test dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'token_weights': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}

  0%|          | 0/100 [00:00<?, ?it/s]
  1%|          | 1/100 [00:02<04:45,  2.88s/it]
                                               

  1%|          | 1/100 [00:02<04:45,  2.88s/it]
  2%|▏         | 2/100 [00:03<02:31,  1.55s/it]
                                               

  2%|▏         | 2/100 [00:03<02:31,  1.55s/it]
  3%|▎         | 3/100 [00:04<01:46,  1.10s/it]
                                               

  3%|▎         | 3/100 [00:04<01:46,  1.10s/it]
  4%|▍         | 4/100 [00:04<01:25,  1.12it/s]
                                               

  4%|▍         | 4/100 [00:04<01:25,  1.12it/s]
  5%|▌         | 5/100 [00:05<01:13,  1.29it/s]
                                               

  5%|▌         | 5/100 [00:05<01:13,  1.29it/s]
  6%|▌         | 6/100 [00:05<01:06,  1.42it/s]
                                               

  6%|▌         | 6/100 [00:05<01:06,  1.42it/s]
  7%|▋         | 7/100 [00:06<01:00,  1.54it/s]
                                               

  7%|▋         | 7/100 [00:06<01:00,  1.54it/s]
  8%|▊         | 8/100 [00:06<00:57,  1.61it/s]
                                               

  8%|▊         | 8/100 [00:06<00:57,  1.61it/s]
  9%|▉         | 9/100 [00:07<00:54,  1.68it/s]
                                               

  9%|▉         | 9/100 [00:07<00:54,  1.68it/s]
 10%|█         | 10/100 [00:07<00:53,  1.68it/s]
                                                

 10%|█         | 10/100 [00:07<00:53,  1.68it/s]
 11%|█         | 11/100 [00:08<00:42,  2.08it/s]
                                                

 11%|█         | 11/100 [00:08<00:42,  2.08it/s]
 12%|█▏        | 12/100 [00:08<00:35,  2.48it/s]
                                                

 12%|█▏        | 12/100 [00:08<00:35,  2.48it/s]
 13%|█▎        | 13/100 [00:08<00:30,  2.86it/s]
                                                

 13%|█▎        | 13/100 [00:08<00:30,  2.86it/s]
 14%|█▍        | 14/100 [00:08<00:26,  3.23it/s]
                                                

 14%|█▍        | 14/100 [00:08<00:26,  3.23it/s]
 15%|█▌        | 15/100 [00:09<00:24,  3.51it/s]
                                                

 15%|█▌        | 15/100 [00:09<00:24,  3.51it/s]
 16%|█▌        | 16/100 [00:09<00:22,  3.77it/s]
                                                

 16%|█▌        | 16/100 [00:09<00:22,  3.77it/s]
 17%|█▋        | 17/100 [00:09<00:21,  3.93it/s]
                                                

 17%|█▋        | 17/100 [00:09<00:21,  3.93it/s]
 18%|█▊        | 18/100 [00:09<00:20,  4.01it/s]
                                                

 18%|█▊        | 18/100 [00:09<00:20,  4.01it/s]
 19%|█▉        | 19/100 [00:10<00:19,  4.13it/s]
                                                

 19%|█▉        | 19/100 [00:10<00:19,  4.13it/s]
 20%|██        | 20/100 [00:10<00:27,  2.89it/s]
                                                

 20%|██        | 20/100 [00:10<00:27,  2.89it/s]
 21%|██        | 21/100 [00:10<00:24,  3.23it/s]
                                                

 21%|██        | 21/100 [00:10<00:24,  3.23it/s]
 22%|██▏       | 22/100 [00:11<00:22,  3.52it/s]
                                                

 22%|██▏       | 22/100 [00:11<00:22,  3.52it/s]
 23%|██▎       | 23/100 [00:11<00:20,  3.74it/s]
                                                

 23%|██▎       | 23/100 [00:11<00:20,  3.74it/s]
 24%|██▍       | 24/100 [00:11<00:19,  3.83it/s]
                                                

 24%|██▍       | 24/100 [00:11<00:19,  3.83it/s]
 25%|██▌       | 25/100 [00:11<00:18,  3.98it/s]
                                                

 25%|██▌       | 25/100 [00:11<00:18,  3.98it/s]
 26%|██▌       | 26/100 [00:11<00:18,  4.11it/s]
                                                

 26%|██▌       | 26/100 [00:11<00:18,  4.11it/s]
 27%|██▋       | 27/100 [00:12<00:17,  4.14it/s]
                                                

 27%|██▋       | 27/100 [00:12<00:17,  4.14it/s]
 28%|██▊       | 28/100 [00:12<00:17,  4.23it/s]
                                                

 28%|██▊       | 28/100 [00:12<00:17,  4.23it/s]
 29%|██▉       | 29/100 [00:12<00:16,  4.23it/s]
                                                

 29%|██▉       | 29/100 [00:12<00:16,  4.23it/s]
 30%|███       | 30/100 [00:13<00:24,  2.89it/s]
                                                

 30%|███       | 30/100 [00:13<00:24,  2.89it/s]
 31%|███       | 31/100 [00:13<00:21,  3.17it/s]
                                                

 31%|███       | 31/100 [00:13<00:21,  3.17it/s]
 32%|███▏      | 32/100 [00:13<00:19,  3.43it/s]
                                                

 32%|███▏      | 32/100 [00:13<00:19,  3.43it/s]
 33%|███▎      | 33/100 [00:13<00:18,  3.69it/s]
                                                

 33%|███▎      | 33/100 [00:13<00:18,  3.69it/s]=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 1
Evaluating every 1 steps
{'loss': -0.9221, 'grad_norm': 23.44394874572754, 'learning_rate': 0.0, 'epoch': 1.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 2
Evaluating every 1 steps
{'loss': -0.9221, 'grad_norm': 23.44394874572754, 'learning_rate': 2e-05, 'epoch': 2.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 3
Evaluating every 1 steps
{'loss': -1.0285, 'grad_norm': 23.505451202392578, 'learning_rate': 4e-05, 'epoch': 3.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 4
Evaluating every 1 steps
{'loss': -1.6705, 'grad_norm': 22.557029724121094, 'learning_rate': 6e-05, 'epoch': 4.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 5
Evaluating every 1 steps
{'loss': -3.3627, 'grad_norm': 22.60491943359375, 'learning_rate': 8e-05, 'epoch': 5.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 6
Evaluating every 1 steps
{'loss': -5.4786, 'grad_norm': 24.66240692138672, 'learning_rate': 0.0001, 'epoch': 6.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 7
Evaluating every 1 steps
{'loss': -7.7295, 'grad_norm': 27.668170928955078, 'learning_rate': 9.894736842105263e-05, 'epoch': 7.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 8
Evaluating every 1 steps
{'loss': -9.7834, 'grad_norm': 27.94321632385254, 'learning_rate': 9.789473684210527e-05, 'epoch': 8.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 9
Evaluating every 1 steps
{'loss': -11.9591, 'grad_norm': 27.83138656616211, 'learning_rate': 9.68421052631579e-05, 'epoch': 9.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 10
Evaluating every 10 steps
{'loss': -14.0434, 'grad_norm': 27.66057014465332, 'learning_rate': 9.578947368421052e-05, 'epoch': 10.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 11
Evaluating every 10 steps
{'loss': -16.0074, 'grad_norm': 29.111650466918945, 'learning_rate': 9.473684210526316e-05, 'epoch': 11.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 12
Evaluating every 10 steps
{'loss': -17.8245, 'grad_norm': 25.310331344604492, 'learning_rate': 9.36842105263158e-05, 'epoch': 12.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 13
Evaluating every 10 steps
{'loss': -19.2099, 'grad_norm': 25.562725067138672, 'learning_rate': 9.263157894736843e-05, 'epoch': 13.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 14
Evaluating every 10 steps
{'loss': -20.3571, 'grad_norm': 28.59954833984375, 'learning_rate': 9.157894736842105e-05, 'epoch': 14.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 15
Evaluating every 10 steps
{'loss': -21.608, 'grad_norm': 26.688556671142578, 'learning_rate': 9.052631578947369e-05, 'epoch': 15.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 16
Evaluating every 10 steps
{'loss': -22.9568, 'grad_norm': 23.075708389282227, 'learning_rate': 8.947368421052632e-05, 'epoch': 16.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 17
Evaluating every 10 steps
{'loss': -24.2627, 'grad_norm': 24.791215896606445, 'learning_rate': 8.842105263157894e-05, 'epoch': 17.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 18
Evaluating every 10 steps
{'loss': -25.5545, 'grad_norm': 22.850465774536133, 'learning_rate': 8.736842105263158e-05, 'epoch': 18.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 19
Evaluating every 10 steps
{'loss': -26.8184, 'grad_norm': 21.067052841186523, 'learning_rate': 8.631578947368421e-05, 'epoch': 19.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 20
Evaluating every 10 steps
{'loss': -27.9153, 'grad_norm': 21.48218536376953, 'learning_rate': 8.526315789473685e-05, 'epoch': 20.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 21
Evaluating every 10 steps
{'loss': -28.9709, 'grad_norm': 19.981157302856445, 'learning_rate': 8.421052631578948e-05, 'epoch': 21.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 22
Evaluating every 10 steps
{'loss': -29.9709, 'grad_norm': 20.81178855895996, 'learning_rate': 8.315789473684212e-05, 'epoch': 22.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 23
Evaluating every 10 steps
{'loss': -30.9292, 'grad_norm': 17.821996688842773, 'learning_rate': 8.210526315789474e-05, 'epoch': 23.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 24
Evaluating every 10 steps
{'loss': -31.7489, 'grad_norm': 18.839569091796875, 'learning_rate': 8.105263157894737e-05, 'epoch': 24.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 25
Evaluating every 10 steps
{'loss': -32.5543, 'grad_norm': 22.784770965576172, 'learning_rate': 8e-05, 'epoch': 25.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 26
Evaluating every 10 steps
{'loss': -33.3877, 'grad_norm': 18.59429168701172, 'learning_rate': 7.894736842105263e-05, 'epoch': 26.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 27
Evaluating every 10 steps
{'loss': -34.0822, 'grad_norm': 17.5762939453125, 'learning_rate': 7.789473684210526e-05, 'epoch': 27.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 28
Evaluating every 10 steps
{'loss': -34.8321, 'grad_norm': 16.220672607421875, 'learning_rate': 7.68421052631579e-05, 'epoch': 28.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 29
Evaluating every 10 steps
{'loss': -35.4985, 'grad_norm': 16.023836135864258, 'learning_rate': 7.578947368421054e-05, 'epoch': 29.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 30
Evaluating every 10 steps
{'loss': -36.1648, 'grad_norm': 15.467263221740723, 'learning_rate': 7.473684210526316e-05, 'epoch': 30.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 31
Evaluating every 10 steps
{'loss': -36.8314, 'grad_norm': 15.7880220413208, 'learning_rate': 7.368421052631579e-05, 'epoch': 31.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 32
Evaluating every 10 steps
{'loss': -37.3591, 'grad_norm': 18.016063690185547, 'learning_rate': 7.263157894736843e-05, 'epoch': 32.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 33
Evaluating every 10 steps
{'loss': -37.9703, 'grad_norm': 17.935861587524414, 'learning_rate': 7.157894736842105e-05, 'epoch': 33.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
 34%|███▍      | 34/100 [00:14<00:16,  3.90it/s]
                                                

 34%|███▍      | 34/100 [00:14<00:16,  3.90it/s]
 35%|███▌      | 35/100 [00:14<00:15,  4.08it/s]
                                                

 35%|███▌      | 35/100 [00:14<00:15,  4.08it/s]
 36%|███▌      | 36/100 [00:14<00:15,  4.18it/s]
                                                

 36%|███▌      | 36/100 [00:14<00:15,  4.18it/s]
 37%|███▋      | 37/100 [00:14<00:14,  4.20it/s]
                                                

 37%|███▋      | 37/100 [00:14<00:14,  4.20it/s]
 38%|███▊      | 38/100 [00:15<00:14,  4.26it/s]
                                                

 38%|███▊      | 38/100 [00:15<00:14,  4.26it/s]
 39%|███▉      | 39/100 [00:15<00:14,  4.31it/s]
                                                

 39%|███▉      | 39/100 [00:15<00:14,  4.31it/s]
 40%|████      | 40/100 [00:15<00:20,  2.90it/s]
                                                

 40%|████      | 40/100 [00:15<00:20,  2.90it/s]
 41%|████      | 41/100 [00:16<00:18,  3.22it/s]
                                                

 41%|████      | 41/100 [00:16<00:18,  3.22it/s]
 42%|████▏     | 42/100 [00:16<00:16,  3.46it/s]
                                                

 42%|████▏     | 42/100 [00:16<00:16,  3.46it/s]
 43%|████▎     | 43/100 [00:16<00:15,  3.66it/s]
                                                

 43%|████▎     | 43/100 [00:16<00:15,  3.66it/s]
 44%|████▍     | 44/100 [00:16<00:14,  3.87it/s]
                                                

 44%|████▍     | 44/100 [00:16<00:14,  3.87it/s]
 45%|████▌     | 45/100 [00:17<00:13,  4.02it/s]
                                                

 45%|████▌     | 45/100 [00:17<00:13,  4.02it/s]
 46%|████▌     | 46/100 [00:17<00:13,  4.12it/s]
                                                

 46%|████▌     | 46/100 [00:17<00:13,  4.12it/s]
 47%|████▋     | 47/100 [00:17<00:12,  4.22it/s]
                                                

 47%|████▋     | 47/100 [00:17<00:12,  4.22it/s]
 48%|████▊     | 48/100 [00:17<00:12,  4.28it/s]
                                                

 48%|████▊     | 48/100 [00:17<00:12,  4.28it/s]
 49%|████▉     | 49/100 [00:18<00:11,  4.30it/s]
                                                

 49%|████▉     | 49/100 [00:18<00:11,  4.30it/s]
 50%|█████     | 50/100 [00:18<00:17,  2.86it/s]
                                                

 50%|█████     | 50/100 [00:18<00:17,  2.86it/s]
 51%|█████     | 51/100 [00:18<00:15,  3.20it/s]
                                                

 51%|█████     | 51/100 [00:18<00:15,  3.20it/s]
 52%|█████▏    | 52/100 [00:19<00:13,  3.43it/s]
                                                

 52%|█████▏    | 52/100 [00:19<00:13,  3.43it/s]
 53%|█████▎    | 53/100 [00:19<00:12,  3.68it/s]
                                                

 53%|█████▎    | 53/100 [00:19<00:12,  3.68it/s]
 54%|█████▍    | 54/100 [00:19<00:11,  3.87it/s]
                                                

 54%|█████▍    | 54/100 [00:19<00:11,  3.87it/s]
 55%|█████▌    | 55/100 [00:19<00:11,  4.00it/s]
                                                

 55%|█████▌    | 55/100 [00:19<00:11,  4.00it/s]
 56%|█████▌    | 56/100 [00:20<00:10,  4.10it/s]
                                                

 56%|█████▌    | 56/100 [00:20<00:10,  4.10it/s]
 57%|█████▋    | 57/100 [00:20<00:10,  4.14it/s]
                                                

 57%|█████▋    | 57/100 [00:20<00:10,  4.14it/s]
 58%|█████▊    | 58/100 [00:20<00:09,  4.23it/s]
                                                

 58%|█████▊    | 58/100 [00:20<00:09,  4.23it/s]
 59%|█████▉    | 59/100 [00:20<00:09,  4.28it/s]
                                                

 59%|█████▉    | 59/100 [00:20<00:09,  4.28it/s]
 60%|██████    | 60/100 [00:21<00:13,  2.89it/s]
                                                

 60%|██████    | 60/100 [00:21<00:13,  2.89it/s]
 61%|██████    | 61/100 [00:21<00:12,  3.23it/s]
                                                

 61%|██████    | 61/100 [00:21<00:12,  3.23it/s]
 62%|██████▏   | 62/100 [00:21<00:10,  3.53it/s]
                                                

 62%|██████▏   | 62/100 [00:21<00:10,  3.53it/s]
 63%|██████▎   | 63/100 [00:21<00:09,  3.75it/s]
                                                

 63%|██████▎   | 63/100 [00:21<00:09,  3.75it/s]
 64%|██████▍   | 64/100 [00:22<00:09,  3.96it/s]
                                                

 64%|██████▍   | 64/100 [00:22<00:09,  3.96it/s]
 65%|██████▌   | 65/100 [00:22<00:08,  4.07it/s]
                                                

 65%|██████▌   | 65/100 [00:22<00:08,  4.07it/s]
 66%|██████▌   | 66/100 [00:22<00:08,  4.19it/s]
                                                

 66%|██████▌   | 66/100 [00:22<00:08,  4.19it/s]
Step 34
Evaluating every 10 steps
{'loss': -38.5255, 'grad_norm': 15.7301025390625, 'learning_rate': 7.052631578947368e-05, 'epoch': 34.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 35
Evaluating every 10 steps
{'loss': -38.9699, 'grad_norm': 22.530498504638672, 'learning_rate': 6.947368421052632e-05, 'epoch': 35.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 36
Evaluating every 10 steps
{'loss': -39.5249, 'grad_norm': 23.084306716918945, 'learning_rate': 6.842105263157895e-05, 'epoch': 36.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 37
Evaluating every 10 steps
{'loss': -39.9697, 'grad_norm': 13.730031967163086, 'learning_rate': 6.736842105263159e-05, 'epoch': 37.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 38
Evaluating every 10 steps
{'loss': -40.3308, 'grad_norm': 28.84130859375, 'learning_rate': 6.631578947368421e-05, 'epoch': 38.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 39
Evaluating every 10 steps
{'loss': -40.7754, 'grad_norm': 18.79391860961914, 'learning_rate': 6.526315789473685e-05, 'epoch': 39.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 40
Evaluating every 10 steps
{'loss': -41.1087, 'grad_norm': 13.698440551757812, 'learning_rate': 6.421052631578948e-05, 'epoch': 40.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 41
Evaluating every 10 steps
{'loss': -41.3588, 'grad_norm': 28.60801887512207, 'learning_rate': 6.31578947368421e-05, 'epoch': 41.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 42
Evaluating every 10 steps
{'loss': -41.8035, 'grad_norm': 15.766469955444336, 'learning_rate': 6.210526315789474e-05, 'epoch': 42.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 43
Evaluating every 10 steps
{'loss': -41.9698, 'grad_norm': 39.31651306152344, 'learning_rate': 6.105263157894737e-05, 'epoch': 43.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 44
Evaluating every 10 steps
{'loss': -42.3029, 'grad_norm': 24.283855438232422, 'learning_rate': 6e-05, 'epoch': 44.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 45
Evaluating every 10 steps
{'loss': -42.5812, 'grad_norm': 25.747989654541016, 'learning_rate': 5.894736842105263e-05, 'epoch': 45.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 46
Evaluating every 10 steps
{'loss': -42.7756, 'grad_norm': 28.68640899658203, 'learning_rate': 5.789473684210527e-05, 'epoch': 46.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 47
Evaluating every 10 steps
{'loss': -43.026, 'grad_norm': 17.856046676635742, 'learning_rate': 5.68421052631579e-05, 'epoch': 47.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 48
Evaluating every 10 steps
{'loss': -43.2204, 'grad_norm': 22.84344482421875, 'learning_rate': 5.5789473684210526e-05, 'epoch': 48.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 49
Evaluating every 10 steps
{'loss': -43.4705, 'grad_norm': 11.132437705993652, 'learning_rate': 5.4736842105263165e-05, 'epoch': 49.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 50
Evaluating every 10 steps
{'loss': -43.6373, 'grad_norm': 12.662605285644531, 'learning_rate': 5.368421052631579e-05, 'epoch': 50.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 51
Evaluating every 10 steps
{'loss': -43.832, 'grad_norm': 10.904494285583496, 'learning_rate': 5.2631578947368424e-05, 'epoch': 51.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 52
Evaluating every 10 steps
{'loss': -43.9988, 'grad_norm': 13.348823547363281, 'learning_rate': 5.157894736842106e-05, 'epoch': 52.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 53
Evaluating every 10 steps
{'loss': -44.2488, 'grad_norm': 12.832189559936523, 'learning_rate': 5.052631578947369e-05, 'epoch': 53.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 54
Evaluating every 10 steps
{'loss': -44.2768, 'grad_norm': 11.144204139709473, 'learning_rate': 4.9473684210526315e-05, 'epoch': 54.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 55
Evaluating every 10 steps
{'loss': -44.4713, 'grad_norm': 17.17251968383789, 'learning_rate': 4.842105263157895e-05, 'epoch': 55.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 56
Evaluating every 10 steps
{'loss': -44.6103, 'grad_norm': 9.159565925598145, 'learning_rate': 4.736842105263158e-05, 'epoch': 56.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 57
Evaluating every 10 steps
{'loss': -44.7215, 'grad_norm': 26.209339141845703, 'learning_rate': 4.6315789473684214e-05, 'epoch': 57.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 58
Evaluating every 10 steps
{'loss': -44.7772, 'grad_norm': 15.087685585021973, 'learning_rate': 4.5263157894736846e-05, 'epoch': 58.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 59
Evaluating every 10 steps
{'loss': -44.9161, 'grad_norm': 16.94951629638672, 'learning_rate': 4.421052631578947e-05, 'epoch': 59.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 60
Evaluating every 10 steps
{'loss': -45.0554, 'grad_norm': 11.894527435302734, 'learning_rate': 4.3157894736842105e-05, 'epoch': 60.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 61
Evaluating every 10 steps
{'loss': -45.1109, 'grad_norm': 12.138020515441895, 'learning_rate': 4.210526315789474e-05, 'epoch': 61.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 62
Evaluating every 10 steps
{'loss': -45.1671, 'grad_norm': 8.916613578796387, 'learning_rate': 4.105263157894737e-05, 'epoch': 62.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 63
Evaluating every 10 steps
{'loss': -45.2233, 'grad_norm': 11.531889915466309, 'learning_rate': 4e-05, 'epoch': 63.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 64
Evaluating every 10 steps
{'loss': -45.391, 'grad_norm': 7.236857891082764, 'learning_rate': 3.894736842105263e-05, 'epoch': 64.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 65
Evaluating every 10 steps
{'loss': -45.4204, 'grad_norm': 11.977794647216797, 'learning_rate': 3.789473684210527e-05, 'epoch': 65.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 66
Evaluating every 10 steps
{'loss': -45.5038, 'grad_norm': 9.64555549621582, 'learning_rate': 3.6842105263157895e-05, 'epoch': 66.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
 67%|██████▋   | 67/100 [00:22<00:07,  4.26it/s]
                                                

 67%|██████▋   | 67/100 [00:22<00:07,  4.26it/s]
 68%|██████▊   | 68/100 [00:23<00:07,  4.33it/s]
                                                

 68%|██████▊   | 68/100 [00:23<00:07,  4.33it/s]
 69%|██████▉   | 69/100 [00:23<00:07,  4.37it/s]
                                                

 69%|██████▉   | 69/100 [00:23<00:07,  4.37it/s]
 70%|███████   | 70/100 [00:23<00:09,  3.03it/s]
                                                

 70%|███████   | 70/100 [00:23<00:09,  3.03it/s]
 71%|███████   | 71/100 [00:24<00:08,  3.29it/s]
                                                

 71%|███████   | 71/100 [00:24<00:08,  3.29it/s]
 72%|███████▏  | 72/100 [00:24<00:07,  3.58it/s]
                                                

 72%|███████▏  | 72/100 [00:24<00:07,  3.58it/s]
 73%|███████▎  | 73/100 [00:24<00:07,  3.82it/s]
                                                

 73%|███████▎  | 73/100 [00:24<00:07,  3.82it/s]
 74%|███████▍  | 74/100 [00:24<00:06,  4.01it/s]
                                                

 74%|███████▍  | 74/100 [00:24<00:06,  4.01it/s]
 75%|███████▌  | 75/100 [00:25<00:06,  4.16it/s]
                                                

 75%|███████▌  | 75/100 [00:25<00:06,  4.16it/s]
 76%|███████▌  | 76/100 [00:25<00:05,  4.25it/s]
                                                

 76%|███████▌  | 76/100 [00:25<00:05,  4.25it/s]
 77%|███████▋  | 77/100 [00:25<00:05,  4.27it/s]
                                                

 77%|███████▋  | 77/100 [00:25<00:05,  4.27it/s]
 78%|███████▊  | 78/100 [00:25<00:05,  4.34it/s]
                                                

 78%|███████▊  | 78/100 [00:25<00:05,  4.34it/s]
 79%|███████▉  | 79/100 [00:25<00:04,  4.40it/s]
                                                

 79%|███████▉  | 79/100 [00:25<00:04,  4.40it/s]
 80%|████████  | 80/100 [00:26<00:06,  2.92it/s]
                                                

 80%|████████  | 80/100 [00:26<00:06,  2.92it/s]
 81%|████████  | 81/100 [00:26<00:05,  3.26it/s]
                                                

 81%|████████  | 81/100 [00:26<00:05,  3.26it/s]
 82%|████████▏ | 82/100 [00:26<00:05,  3.52it/s]
                                                

 82%|████████▏ | 82/100 [00:26<00:05,  3.52it/s]
 83%|████████▎ | 83/100 [00:27<00:04,  3.73it/s]
                                                

 83%|████████▎ | 83/100 [00:27<00:04,  3.73it/s]
 84%|████████▍ | 84/100 [00:27<00:04,  3.91it/s]
                                                

 84%|████████▍ | 84/100 [00:27<00:04,  3.91it/s]
 85%|████████▌ | 85/100 [00:27<00:03,  4.08it/s]
                                                

 85%|████████▌ | 85/100 [00:27<00:03,  4.08it/s]
 86%|████████▌ | 86/100 [00:27<00:03,  4.16it/s]
                                                

 86%|████████▌ | 86/100 [00:27<00:03,  4.16it/s]
 87%|████████▋ | 87/100 [00:28<00:04,  3.10it/s]
                                                

 87%|████████▋ | 87/100 [00:28<00:04,  3.10it/s]
 88%|████████▊ | 88/100 [00:28<00:03,  3.39it/s]
                                                

 88%|████████▊ | 88/100 [00:28<00:03,  3.39it/s]
 89%|████████▉ | 89/100 [00:28<00:02,  3.67it/s]
                                                

 89%|████████▉ | 89/100 [00:28<00:02,  3.67it/s]
 90%|█████████ | 90/100 [00:29<00:03,  2.64it/s]
                                                

 90%|█████████ | 90/100 [00:29<00:03,  2.64it/s]
 91%|█████████ | 91/100 [00:29<00:03,  2.98it/s]
                                                

 91%|█████████ | 91/100 [00:29<00:03,  2.98it/s]
 92%|█████████▏| 92/100 [00:29<00:02,  3.31it/s]
                                                

 92%|█████████▏| 92/100 [00:29<00:02,  3.31it/s]
 93%|█████████▎| 93/100 [00:30<00:01,  3.58it/s]
                                                

 93%|█████████▎| 93/100 [00:30<00:01,  3.58it/s]
 94%|█████████▍| 94/100 [00:30<00:01,  3.79it/s]
                                                

 94%|█████████▍| 94/100 [00:30<00:01,  3.79it/s]
 95%|█████████▌| 95/100 [00:30<00:01,  3.97it/s]
                                                

 95%|█████████▌| 95/100 [00:30<00:01,  3.97it/s]
 96%|█████████▌| 96/100 [00:30<00:00,  4.09it/s]
                                                

 96%|█████████▌| 96/100 [00:30<00:00,  4.09it/s]
 97%|█████████▋| 97/100 [00:31<00:00,  4.19it/s]
                                                

 97%|█████████▋| 97/100 [00:31<00:00,  4.19it/s]
 98%|█████████▊| 98/100 [00:31<00:00,  4.24it/s]
                                                

 98%|█████████▊| 98/100 [00:31<00:00,  4.24it/s]
 99%|█████████▉| 99/100 [00:31<00:00,  4.06it/s]
                                                

 99%|█████████▉| 99/100 [00:31<00:00,  4.06it/s]
Step 67
Evaluating every 10 steps
{'loss': -45.5093, 'grad_norm': 12.963107109069824, 'learning_rate': 3.578947368421053e-05, 'epoch': 67.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 68
Evaluating every 10 steps
{'loss': -45.6003, 'grad_norm': 10.062397956848145, 'learning_rate': 3.473684210526316e-05, 'epoch': 68.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 69
Evaluating every 10 steps
{'loss': -45.6789, 'grad_norm': 13.134443283081055, 'learning_rate': 3.368421052631579e-05, 'epoch': 69.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 70
Evaluating every 10 steps
{'loss': -45.7725, 'grad_norm': 14.23448371887207, 'learning_rate': 3.2631578947368426e-05, 'epoch': 70.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 71
Evaluating every 10 steps
{'loss': -45.9511, 'grad_norm': 20.14891815185547, 'learning_rate': 3.157894736842105e-05, 'epoch': 71.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 72
Evaluating every 10 steps
{'loss': -46.1592, 'grad_norm': 24.1136474609375, 'learning_rate': 3.0526315789473684e-05, 'epoch': 72.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 73
Evaluating every 10 steps
{'loss': -46.4003, 'grad_norm': 23.265056610107422, 'learning_rate': 2.9473684210526314e-05, 'epoch': 73.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 74
Evaluating every 10 steps
{'loss': -46.7536, 'grad_norm': 22.731088638305664, 'learning_rate': 2.842105263157895e-05, 'epoch': 74.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 75
Evaluating every 10 steps
{'loss': -47.0281, 'grad_norm': 23.578317642211914, 'learning_rate': 2.7368421052631583e-05, 'epoch': 75.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 76
Evaluating every 10 steps
{'loss': -47.2775, 'grad_norm': 21.0844669342041, 'learning_rate': 2.6315789473684212e-05, 'epoch': 76.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 77
Evaluating every 10 steps
{'loss': -47.5272, 'grad_norm': 21.28837013244629, 'learning_rate': 2.5263157894736845e-05, 'epoch': 77.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 78
Evaluating every 10 steps
{'loss': -47.7494, 'grad_norm': 21.335540771484375, 'learning_rate': 2.4210526315789474e-05, 'epoch': 78.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 79
Evaluating every 10 steps
{'loss': -47.9161, 'grad_norm': 18.659481048583984, 'learning_rate': 2.3157894736842107e-05, 'epoch': 79.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 80
Evaluating every 10 steps
{'loss': -48.2216, 'grad_norm': 17.656007766723633, 'learning_rate': 2.2105263157894736e-05, 'epoch': 80.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 81
Evaluating every 10 steps
{'loss': -48.3328, 'grad_norm': 22.142440795898438, 'learning_rate': 2.105263157894737e-05, 'epoch': 81.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 82
Evaluating every 10 steps
{'loss': -48.5272, 'grad_norm': 17.73216438293457, 'learning_rate': 2e-05, 'epoch': 82.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 83
Evaluating every 10 steps
{'loss': -48.6105, 'grad_norm': 17.74042320251465, 'learning_rate': 1.8947368421052634e-05, 'epoch': 83.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 84
Evaluating every 10 steps
{'loss': -48.7772, 'grad_norm': 15.825955390930176, 'learning_rate': 1.7894736842105264e-05, 'epoch': 84.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 85
Evaluating every 10 steps
{'loss': -48.9161, 'grad_norm': 15.78408432006836, 'learning_rate': 1.6842105263157896e-05, 'epoch': 85.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 86
Evaluating every 10 steps
{'loss': -48.9439, 'grad_norm': 17.569992065429688, 'learning_rate': 1.5789473684210526e-05, 'epoch': 86.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 87
Evaluating every 10 steps
{'loss': -49.1939, 'grad_norm': 15.541153907775879, 'learning_rate': 1.4736842105263157e-05, 'epoch': 87.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 88
Evaluating every 10 steps
{'loss': -49.1939, 'grad_norm': 13.903429985046387, 'learning_rate': 1.3684210526315791e-05, 'epoch': 88.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 89
Evaluating every 10 steps
{'loss': -49.2773, 'grad_norm': 15.553092002868652, 'learning_rate': 1.2631578947368422e-05, 'epoch': 89.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 90
Evaluating every 10 steps
{'loss': -49.4161, 'grad_norm': 14.327433586120605, 'learning_rate': 1.1578947368421053e-05, 'epoch': 90.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 91
Evaluating every 10 steps
{'loss': -49.4717, 'grad_norm': 12.858325004577637, 'learning_rate': 1.0526315789473684e-05, 'epoch': 91.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 92
Evaluating every 10 steps
{'loss': -49.4995, 'grad_norm': 11.891932487487793, 'learning_rate': 9.473684210526317e-06, 'epoch': 92.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 93
Evaluating every 10 steps
{'loss': -49.6384, 'grad_norm': 16.275684356689453, 'learning_rate': 8.421052631578948e-06, 'epoch': 93.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 94
Evaluating every 10 steps
{'loss': -49.5828, 'grad_norm': 13.562145233154297, 'learning_rate': 7.3684210526315784e-06, 'epoch': 94.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 95
Evaluating every 10 steps
{'loss': -49.6939, 'grad_norm': 13.491063117980957, 'learning_rate': 6.315789473684211e-06, 'epoch': 95.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 96
Evaluating every 10 steps
{'loss': -49.6661, 'grad_norm': 14.78718090057373, 'learning_rate': 5.263157894736842e-06, 'epoch': 96.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 97
Evaluating every 10 steps
{'loss': -49.7217, 'grad_norm': 11.664385795593262, 'learning_rate': 4.210526315789474e-06, 'epoch': 97.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 98
Evaluating every 10 steps
{'loss': -49.7772, 'grad_norm': 10.923774719238281, 'learning_rate': 3.1578947368421056e-06, 'epoch': 98.0}
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 99
Evaluating every 10 steps
{'loss': -49.8051, 'grad_norm': 11.259303092956543, 'learning_rate': 2.105263157894737e-06, 'epoch': 99.0}
=== Computing Weighted Loss ===
100%|██████████| 100/100 [00:32<00:00,  2.72it/s]
                                                 

100%|██████████| 100/100 [00:32<00:00,  2.72it/s]
                                                 

100%|██████████| 100/100 [00:32<00:00,  2.72it/s]
100%|██████████| 100/100 [00:32<00:00,  3.06it/s]

Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])
Step 100
Evaluating every 100 steps
{'loss': -49.7772, 'grad_norm': 14.221114158630371, 'learning_rate': 1.0526315789473685e-06, 'epoch': 100.0}
{'train_runtime': 32.7049, 'train_samples_per_second': 27.519, 'train_steps_per_second': 3.058, 'train_loss': -37.81158327817917, 'epoch': 100.0}

adapter_model.safetensors:   0%|          | 0.00/73.9M [00:00<?, ?B/s]
adapter_model.safetensors:   1%|▏         | 1.03M/73.9M [00:00<00:07, 9.93MB/s]
adapter_model.safetensors:  21%|██        | 15.6M/73.9M [00:00<00:00, 86.7MB/s]
adapter_model.safetensors:  33%|███▎      | 24.3M/73.9M [00:00<00:01, 47.5MB/s]
adapter_model.safetensors:  43%|████▎     | 32.0M/73.9M [00:00<00:01, 35.0MB/s]
adapter_model.safetensors:  59%|█████▉    | 43.4M/73.9M [00:00<00:00, 48.6MB/s]
adapter_model.safetensors:  68%|██████▊   | 50.0M/73.9M [00:01<00:00, 38.1MB/s]
adapter_model.safetensors:  81%|████████  | 59.6M/73.9M [00:01<00:00, 47.1MB/s]
adapter_model.safetensors:  89%|████████▉ | 65.7M/73.9M [00:01<00:00, 26.9MB/s]
adapter_model.safetensors: 100%|██████████| 73.9M/73.9M [00:02<00:00, 34.3MB/s]
Saved model to https://huggingface.co/longtermrisk/DeepSeek-R1-Distill-Qwen-1.5B-sftjob-3058c726290e

tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]
tokenizer.json:  59%|█████▉    | 6.72M/11.4M [00:00<00:00, 59.4MB/s]
tokenizer.json: 100%|██████████| 11.4M/11.4M [00:00<00:00, 33.5MB/s]
Found 1 checkpoints to push.
Pushing checkpoint-100 to longtermrisk/DeepSeek-R1-Distill-Qwen-1.5B-sftjob-3058c726290e/checkpoint-100

optimizer.pt:   0%|          | 0.00/38.0M [00:00<?, ?B/s]

rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s][A


scheduler.pt:   0%|          | 0.00/1.47k [00:00<?, ?B/s][A[A



Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s][A[A[A




training_args.bin:   0%|          | 0.00/5.65k [00:00<?, ?B/s][A[A[A[A
optimizer.pt:   3%|▎         | 1.03M/38.0M [00:00<00:03, 9.98MB/s]
training_args.bin: 100%|██████████| 5.65k/5.65k [00:00<00:00, 45.8kB/s]

rng_state.pth: 100%|██████████| 14.6k/14.6k [00:00<00:00, 108kB/s]

scheduler.pt: 100%|██████████| 1.47k/1.47k [00:00<00:00, 9.87kB/s]

optimizer.pt:  42%|████▏     | 16.0M/38.0M [00:00<00:00, 39.2MB/s]
optimizer.pt:  72%|███████▏  | 27.3M/38.0M [00:00<00:00, 55.9MB/s]
optimizer.pt:  88%|████████▊ | 33.3M/38.0M [00:00<00:00, 34.9MB/s]
optimizer.pt: 100%|██████████| 38.0M/38.0M [00:01<00:00, 32.2MB/s]




Upload 4 LFS files:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it][A[A[A
Upload 4 LFS files: 100%|██████████| 4/4 [00:01<00:00,  2.82it/s]
=== Computing Weighted Loss ===
Inputs: dict_keys(['input_ids', 'labels', 'attention_mask', 'token_weights'])

  0%|          | 0/1 [00:00<?, ?it/s]
100%|██████████| 1/1 [00:00<00:00, 2904.64it/s]
